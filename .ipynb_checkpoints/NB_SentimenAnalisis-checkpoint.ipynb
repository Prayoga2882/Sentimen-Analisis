{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALISIS SENTIMEN OPINI PUBLIK TERHADAP PENERAPAN PEMBELAJARAN LURING PADA MASA PANDEMI COVID-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library dan Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.330000e+18</td>\n",
       "      <td>njmyg_</td>\n",
       "      <td>2020-11-18 23:17:14 SE Asia Standard Time</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>iya sih tapi maksud sender kuliah online kali</td>\n",
       "      <td>['iya', 'sih', 'maksud', 'sender', 'kuliah', '...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.330000e+18</td>\n",
       "      <td>urbbyyyyyyyy</td>\n",
       "      <td>2020-11-18 23:07:06 SE Asia Standard Time</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>males kuliah online temennya sikit</td>\n",
       "      <td>['males', 'kuliah', 'online', 'temennya', 'sik...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.330000e+18</td>\n",
       "      <td>risyaanggun</td>\n",
       "      <td>2020-11-18 23:05:21 SE Asia Standard Time</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tumbenan td kuliah online dosennya minta join ...</td>\n",
       "      <td>['tumben', 'td', 'kuliah', 'online', 'dosen', ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.330000e+18</td>\n",
       "      <td>nyctophilexxx</td>\n",
       "      <td>2020-11-18 22:58:58 SE Asia Standard Time</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nangis krn kecapean kuliah online</td>\n",
       "      <td>['nang', 'krn', 'cape', 'kuliah', 'online']</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.330000e+18</td>\n",
       "      <td>anisanwl</td>\n",
       "      <td>2020-11-18 22:50:53 SE Asia Standard Time</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>apa hanya aku yang merasa semenjak kuliah onli...</td>\n",
       "      <td>['semenjak', 'kuliah', 'online', 'kerja', 'ota...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id       username                                 created_at  \\\n",
       "0  1.330000e+18         njmyg_  2020-11-18 23:17:14 SE Asia Standard Time   \n",
       "1  1.330000e+18   urbbyyyyyyyy  2020-11-18 23:07:06 SE Asia Standard Time   \n",
       "2  1.330000e+18    risyaanggun  2020-11-18 23:05:21 SE Asia Standard Time   \n",
       "3  1.330000e+18  nyctophilexxx  2020-11-18 22:58:58 SE Asia Standard Time   \n",
       "4  1.330000e+18       anisanwl  2020-11-18 22:50:53 SE Asia Standard Time   \n",
       "\n",
       "   replies_count  retweets_count  likes_count  \\\n",
       "0              0               0            0   \n",
       "1              0               0            0   \n",
       "2              0               0            0   \n",
       "3              1               0            0   \n",
       "4              0               0            1   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0      iya sih tapi maksud sender kuliah online kali   \n",
       "1                 males kuliah online temennya sikit   \n",
       "2  tumbenan td kuliah online dosennya minta join ...   \n",
       "3                  nangis krn kecapean kuliah online   \n",
       "4  apa hanya aku yang merasa semenjak kuliah onli...   \n",
       "\n",
       "                                   text_preprocessed  polarity  \n",
       "0  ['iya', 'sih', 'maksud', 'sender', 'kuliah', '...  positive  \n",
       "1  ['males', 'kuliah', 'online', 'temennya', 'sik...  negative  \n",
       "2  ['tumben', 'td', 'kuliah', 'online', 'dosen', ...  negative  \n",
       "3        ['nang', 'krn', 'cape', 'kuliah', 'online']  negative  \n",
       "4  ['semenjak', 'kuliah', 'online', 'kerja', 'ota...  negative  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load Dataset\n",
    "# data_frame = pd.read_csv('tweets_data.csv')\n",
    "data_frame = pd.read_csv('tweets_data_clean.csv')\n",
    "data_frame = data_frame[:5000]\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 9)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA PROCESSING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2925\n",
       "1    2056\n",
       "0      19\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['polarity'].replace(('neutral', 'positive', 'negative'), (0, 1, 2), inplace=True)\n",
    "data_frame['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_frame['text_preprocessed'].values.tolist()\n",
    "label = data_frame['polarity'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data Train dan Test (80/20) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data training: 4000\n",
      "Jumlah data testing: 1000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, y_train, y_test = train_test_split(data, label, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(f'Jumlah data training: {len(train_X)}')\n",
    "print(f'Jumlah data testing: {len(test_X)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXT PREPROCESSING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text_clean = re.sub(r'@[A-Za-z0-9]+', '', str(text)) # Hapus Mention\n",
    "    text_clean = re.sub(r'#[A-Za-z0-9]+', '', text_clean) # Hapus hashtag\n",
    "    text_clean = re.sub(r'RT[\\s]', '', text_clean) # Hapus RT\n",
    "    text_clean = re.sub(r\"http\\S+\", '', text_clean) # Hapus link\n",
    "    text_clean = re.sub(r'[0-9]+', '', text_clean) # Hapus angka\n",
    "    text_clean = text_clean.replace('\\n', ' ') # Ganti enter ke spasi\n",
    "    text_clean = text_clean.translate(str.maketrans('', '', string.punctuation)) # Hapus tanda baca\n",
    "    text_clean = text_clean.strip(' ') # Hapus spasi tdk jelas\n",
    "    return text_clean\n",
    "\n",
    "def casefoldingText(text):\n",
    "    lwr = text\n",
    "    map(str.lower, lwr)\n",
    "    text = lwr\n",
    "    return text\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "def tokenizingText(text): \n",
    "    text = tokenizer.tokenize(text)\n",
    "                               \n",
    "    return text\n",
    "\n",
    "def filteringText(text):\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered \n",
    "    return text\n",
    "\n",
    "def stemmingText(text): \n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    tweet_Cleaning = cleaningText(tweet)\n",
    "    tweet_CaseFolding = casefoldingText(tweet_Cleaning)\n",
    "    tweet_Tokenizing = tokenizingText(tweet_CaseFolding)\n",
    "    tweet_Filtering = filteringText(tweet_Tokenizing)\n",
    "#     tweet_Stemming = stemmingText(tweet_Filtering)    \n",
    "    return tweet_Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai: 18:19:51\n",
      "Selesai: 18:19:57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "time_jkt = pytz.timezone('Asia/Jakarta')\n",
    "print(\"Mulai:\", datetime.now(time_jkt).strftime(\"%H:%M:%S\"))\n",
    "\n",
    "#preprocess data train\n",
    "preprocessed_text = []\n",
    "for i in range(0, len(train_X)):\n",
    "    preprocessed_text.append(process_tweet(train_X[i]))\n",
    "\n",
    "X_train = preprocessed_text\n",
    "\n",
    "# Pkl_Filename = \"X_train.pkl\"  \n",
    "\n",
    "# with open(Pkl_Filename, 'wb') as file:  \n",
    "#     pickle.dump(X_train, file)\n",
    "\n",
    "\n",
    "#preprocess data test\n",
    "preprocessed_text = []\n",
    "for i in range(0, len(test_X)):\n",
    "    preprocessed_text.append(process_tweet(test_X[i]))\n",
    "\n",
    "X_test = preprocessed_text\n",
    "\n",
    "# Pkl_Filename = \"y_train.pkl\"  \n",
    "\n",
    "# with open(Pkl_Filename, 'wb') as file:  \n",
    "#     pickle.dump(y_train, file)\n",
    "print(\"Selesai:\", datetime.now(time_jkt).strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pkl_Filename = \"X_train.pkl\" \n",
    "# with open(Pkl_Filename, 'rb') as file:  \n",
    "#     X_train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pkl_Filename = \"y_train.pkl\" \n",
    "# with open(Pkl_Filename, 'rb') as file:  \n",
    "#     y_train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE DICTIONARY ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictionary(data):\n",
    "  dictionary = dict()\n",
    "  for sampel in  data:\n",
    "    for token in sampel:\n",
    "      dictionary[token] = dictionary.get(token, 0) + 1\n",
    "  #sorting dictionary berdasarkan nilainya\n",
    "  daftar_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "  return dict(daftar_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = createDictionary(X_train)\n",
    "\n",
    "print(\"Token teratas pada Dictionary:\\n\")\n",
    "list(bow.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAIVE BAYES CLASSIFIER ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Navie Bayes Classifier \n",
    "class NBClassifier:\n",
    "\n",
    "    def __init__(self, X_train, y_train, size):  \n",
    "      self.X_train = X_train\n",
    "      self.y_train = y_train\n",
    "      self.size = size\n",
    "\n",
    "    def createDictionary(self):\n",
    "      dictionary = dict()\n",
    "    \n",
    "      for sampel in  X_train:\n",
    "        for token in sampel:\n",
    "          dictionary[token] = dictionary.get(token, 0) + 1\n",
    "      daftar_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "      return dict(daftar_dict)\n",
    "    \n",
    "    def train(self):\n",
    "      X_train_dict = self.createDictionary()\n",
    "      if self.size == 'full':\n",
    "        self.daftar_kata = list(X_train_dict.keys())\n",
    "        self.jumlah_kata = dict.fromkeys(self.daftar_kata, None)\n",
    "        \n",
    "      else:\n",
    "        self.daftar_kata = list(X_train_dict.keys())[:int(self.size)]\n",
    "        self.jumlah_kata = dict.fromkeys(self.daftar_kata, None)\n",
    "            \n",
    "      train = pd.DataFrame(columns = ['X_train', 'y_train'])\n",
    "      train['X_train'] = X_train\n",
    "      train['y_train'] = y_train\n",
    "\n",
    "      train_0 = train.copy()[train['y_train'] == 0]\n",
    "      train_1 = train.copy()[train['y_train'] == 1]\n",
    "      train_2 = train.copy()[train['y_train'] == 2]\n",
    "\n",
    "      Prior_0 = train_0.shape[0]/train.shape[0]\n",
    "      Prior_1 = train_1.shape[0]/train.shape[0]\n",
    "      Prior_2 = train_2.shape[0]/train.shape[0]\n",
    "      \n",
    "      self.Prior = np.array([Prior_0, Prior_1, Prior_2])\n",
    "        \n",
    "      def flat(listOfList):\n",
    "        jadi = []\n",
    "        for elemen in listOfList:\n",
    "          jadi.extend(elemen)\n",
    "        return jadi\n",
    "  \n",
    "      X_train_0 = flat(train[train['y_train'] == 0]['X_train'].tolist())\n",
    "      X_train_1 = flat(train[train['y_train'] == 1]['X_train'].tolist())\n",
    "      X_train_2 = flat(train[train['y_train'] == 2]['X_train'].tolist())\n",
    "    \n",
    "      self.X_train_len = np.array([len(X_train_0), len(X_train_1), len(X_train_2)])\n",
    "\n",
    "      for token in self.daftar_kata:\n",
    "        res = []\n",
    "        res.insert(0, X_train_0.count(token))\n",
    "        res.insert(1, X_train_1.count(token))\n",
    "        res.insert(2, X_train_2.count(token))\n",
    "        self.jumlah_kata[token] = res\n",
    "      return self\n",
    "\n",
    "    def predict(self, X_test):     \n",
    "      pred = []\n",
    "      for sampel in X_test:\n",
    "            \n",
    "        mulai = np.array([1,1,1])\n",
    "\n",
    "        for tokens in sampel:\n",
    "          jumlah_vocab = len(self.daftar_kata)\n",
    "          if tokens in self.daftar_kata:\n",
    "            prob = ((np.array(self.jumlah_kata[tokens])+1) / (self.X_train_len + jumlah_vocab))\n",
    "          else:\n",
    "            prob = ((np.array([0,0,0])+1) / (self.X_train_len + jumlah_vocab))\n",
    "          mulai = mulai * prob\n",
    "        pos = mulai * self.Prior\n",
    "        pred.append(np.argmax(pos))\n",
    "      return pred\n",
    "    \n",
    "    def score(self, pred, labels):\n",
    "      correct = (np.array(pred) == np.array(labels)).sum()\n",
    "      accuracy = correct/len(pred)\n",
    "      return correct, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING DAN TESTING MODEL NB ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NBClassifier at 0x29d59bcb508>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training classifier     \n",
    "nb = NBClassifier(X_train, y_train, 5000)  \n",
    "nb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkl_Filename = \"Pickle_NB_Model.pkl\" \n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    nb = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hasil Evaluasi ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   3   2]\n",
      " [  0 292 109]\n",
      " [  1  65 528]]\n",
      "\n",
      "Accuracy: 0.82\n",
      "\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       0.00      0.00      0.00         5\n",
      "    Positive       0.81      0.73      0.77       401\n",
      "    Negative       0.83      0.89      0.86       594\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.55      0.54      0.54      1000\n",
      "weighted avg       0.82      0.82      0.82      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion=confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, y_pred, target_names=['Neutral', 'Positive', 'Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluasi Performa Model NB Manual\n",
    "cor1, acc1 = nb.score(y_pred, y_test)\n",
    "print(\"Prediksi Benar:\", cor1)\n",
    "print(\"Akurasi: %i / %i = %.4f \" %(cor1, len(y_pred), acc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisasi Tabel ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 3000)\n",
    "hasil = pd.DataFrame(list(zip(test_X, X_test, y_pred)),\n",
    "               columns =['Tweet', 'Tweet_Processed', 'Sentimen'])\n",
    "hasil['Sentimen'].replace((0, 1, 2), ('neutral', 'positive', 'negative'), inplace=True)\n",
    "\n",
    "hasil.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisasi Pie Chart ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "sizes = [count for count in hasil['Sentimen'].value_counts()]\n",
    "labels = list(hasil['Sentimen'].value_counts().index)\n",
    "# explode = (0.1, 0, 0)\n",
    "ax.pie(x = sizes, labels = labels, autopct = '%1.1f%%', textprops={'fontsize': 14})\n",
    "ax.set_title('Polaritas Sentimen Data dari Tweet \\n\\n Jumlah Tweet=10000', fontsize = 16, pad = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisasi WordCloud ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word cloud\n",
    "\n",
    "list_words=''\n",
    "for tweet in hasil['Tweet_Processed']:\n",
    "    for word in tweet:\n",
    "        list_words += ' '+(word)\n",
    "        \n",
    "wordcloud = WordCloud(width = 600, height = 400, background_color = 'black', min_font_size = 10).generate(list_words)\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "ax.set_title('Word Cloud Data Tweet', fontsize = 18)\n",
    "ax.grid(False)\n",
    "ax.imshow((wordcloud))\n",
    "fig.tight_layout(pad=0)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisasi WordCloud Positif ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_pos = hasil.loc[hasil['Sentimen']=='positive']\n",
    "\n",
    "list_words_postive=''\n",
    "for row_word in hasil_pos['Tweet_Processed']:\n",
    "    for word in row_word:\n",
    "        list_words_postive += ' '+(word)\n",
    "        \n",
    "wordcloud = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Greens'\n",
    "                               , min_font_size = 10).generate(list_words_postive)\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "ax.set_title('Word Cloud Data Tweet Positif', fontsize = 18)\n",
    "ax.grid(False)\n",
    "ax.imshow((wordcloud))\n",
    "fig.tight_layout(pad=0)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisasi WordCloud Negatif ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_neg = hasil.loc[hasil['Sentimen']=='negative']\n",
    "\n",
    "list_words_negative=''\n",
    "for row_word in hasil_neg['Tweet_Processed']:\n",
    "    for word in row_word:\n",
    "        list_words_negative += ' '+(word)\n",
    "        \n",
    "wordcloud = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Reds'\n",
    "                               , min_font_size = 10).generate(list_words_negative)\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "ax.set_title('Word Cloud Data Tweet Negatif', fontsize = 18)\n",
    "ax.grid(False)\n",
    "ax.imshow((wordcloud))\n",
    "fig.tight_layout(pad=0)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisasi WordCloud Netral ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_neu = hasil.loc[hasil['Sentimen']=='neutral']\n",
    "\n",
    "list_words_neutral=''\n",
    "for row_word in hasil_neu['Tweet_Processed']:\n",
    "    for word in row_word:\n",
    "        list_words_neutral += ' '+(word)\n",
    "        \n",
    "wordcloud = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Blues_r'\n",
    "                               , min_font_size = 10).generate(list_words_neutral)\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "ax.set_title('Word Cloud Data Tweet Netral', fontsize = 18)\n",
    "ax.grid(False)\n",
    "ax.imshow((wordcloud))\n",
    "fig.tight_layout(pad=0)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_1 = []\n",
    "lagi = ''\n",
    "\n",
    "while lagi != 'n':\n",
    "    tweet_x= input(\"Input tweet: \")\n",
    "    lagi= input(\"Input Lagi (y/n):\")\n",
    "\n",
    "    X_test_1.append(tweet_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = []\n",
    "for i in range(0, len(X_test_1)):\n",
    "    preprocessed_text.append(process_tweet(X_test_1[i]))\n",
    "\n",
    "X_test_2 = preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicting\n",
    "y_pred = nb.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_1 = pd.DataFrame(list(zip(X_test_1, X_test_2, y_pred)),\n",
    "               columns =['Tweet', 'Tweet_Processed', 'Sentimen'])\n",
    "hasil_1['Sentimen'].replace((0, 1, 2), ('neutral', 'positive', 'negative'), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 3000)\n",
    "hasil_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Dari Twitter #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "\n",
    "####Credentials\n",
    "consumer_key='VClbThxnr6T59VVrXtJJ1c7yF'\n",
    "consumer_secret='SKR1Oxo1MTgn6veEEfuEj76nrTaklhtxYP0mlFjHIneJEKubf1'\n",
    "access_token='1362975019131760642-5EjLvg3qNvIAVVa5ui9CO27RWHhWTQ'\n",
    "access_token_secret='8l1NLLiRnd4XIVDtFnoW75GrABPffKB2bR4SgDy5A72VP'\n",
    "\n",
    "# Authenticate\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Set Tokens\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# Instantiate API\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "#Download Tweet\n",
    "KataKunci = input(\"Masukan Kata Kunci:\")\n",
    "JmlTweets = int(input(\"Masukan Jumlah Tweets:\"))\n",
    "searched_tweets = \n",
    "[status for status in tw.Cursor(api.search, q=KataKunci, include_rts=False).items(JmlTweets)]\n",
    "test_tweet = [tweet.text for tweet in searched_tweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = []\n",
    "for i in range(0, len(test_tweet)):\n",
    "    preprocessed_text.append(process_tweet(test_tweet[i]))\n",
    "\n",
    "X_test_tweet = preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "\n",
    "print(\"Mulai:\", datetime.now(time_jkt).strftime(\"%H:%M:%S\"))\n",
    "y_pred = nb.predict(X_test_tweet)\n",
    "\n",
    "#Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisasi Tabel ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_2 = pd.DataFrame(list(zip(test_tweet, X_test_tweet, y_pred)),\n",
    "               columns =['Tweet', 'Tweet_Processed', 'Sentimen'])\n",
    "hasil_2['Sentimen'].replace((0, 1, 2), ('neutral', 'positive', 'negative'), inplace=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 3000)\n",
    "hasil_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisasi Pie Chart ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "sizes = [count for count in hasil_2['Sentimen'].value_counts()]\n",
    "labels = list(hasil_2['Sentimen'].value_counts().index)\n",
    "# explode = (0.1, 0, 0)\n",
    "ax.pie(x = sizes, labels = labels, autopct = '%1.1f%%', textprops={'fontsize': 14})\n",
    "ax.set_title('Polaritas Sentimen Data dari Tweet', fontsize = 16, pad = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi WordCloud ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word cloud\n",
    "\n",
    "list_words=''\n",
    "for tweet in hasil_2['Tweet_Processed']:\n",
    "    for word in tweet:\n",
    "        list_words += ' '+(word)\n",
    "        \n",
    "wordcloud = WordCloud(width = 600, height = 400, background_color = 'black', min_font_size = 10).generate(list_words)\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "ax.set_title('Word Cloud Data Tweet', fontsize = 18)\n",
    "ax.grid(False)\n",
    "ax.imshow((wordcloud))\n",
    "fig.tight_layout(pad=0)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisasi WordCloud Positif ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_pos_2 = hasil_2.loc[hasil_2['Sentimen']=='positive']\n",
    "\n",
    "list_words_postive_2=''\n",
    "for row_word in hasil_pos_2['Tweet_Processed']:\n",
    "    for word in row_word:\n",
    "        list_words_postive_2 += ' '+(word)\n",
    "        \n",
    "wordcloud = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Greens'\n",
    "                               , min_font_size = 10).generate(list_words_postive_2)\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "ax.set_title('Word Cloud Data Tweet Positif', fontsize = 18)\n",
    "ax.grid(False)\n",
    "ax.imshow((wordcloud))\n",
    "fig.tight_layout(pad=0)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisasi WordCloud Negatif ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_neg_2 = hasil_2.loc[hasil_2['Sentimen']=='negative']\n",
    "\n",
    "list_words_negative_2=''\n",
    "for row_word in hasil_neg_2['Tweet_Processed']:\n",
    "    for word in row_word:\n",
    "        list_words_negative_2 += ' '+(word)\n",
    "        \n",
    "wordcloud = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Reds'\n",
    "                               , min_font_size = 10).generate(list_words_negative_2)\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "ax.set_title('Word Cloud Data Tweet Negatif', fontsize = 18)\n",
    "ax.grid(False)\n",
    "ax.imshow((wordcloud))\n",
    "fig.tight_layout(pad=0)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
